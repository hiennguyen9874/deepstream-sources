################################################################################
# Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.
#
# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.
#
################################################################################

This sample is a simple demonstration of using the Triton Inference Server in a
DeepStream graph.

"NvDsTriton" component is used for inferencing via Triton. NvDsTriton component
attaches inference results (object detection in this case) as metadata to the
buffer. This information can be extracted by adding a "INvDsInPlaceDataHandler"
based component which essentially probes the data flowing through a particular
point in the underlying pipeline.

================================================================================
Pre-requisites
================================================================================
- DeepStreamSDK 6.2
- Graph-Composer 2.5.0

===============================================================================
DGPU instructions
===============================================================================
Triton samples for DGPU need to be run in containers based on Triton. This
sample uses the NVIDIA Container Builder to build a container for the sample.

Steps:
* Build the container
$ container_builder -c ds_triton_container_builder_cfg_dgpu.yaml \
      -d target_triton_x86_64.yaml

* Start the container
$ docker run -it --rm  -e DISPLAY=:0 -v /tmp/.X11-unix/:/tmp/.X11-unix \
      --gpus all deepstream_triton_dgpu

===============================================================================
Jetson instructions
===============================================================================
Triton samples for Jetson can be run natively or in a container.

Steps for running natively:
* Setup the Triton Server.
- $ cd /opt/nvidia/deepstream/deepstream/samples
- $ sudo ./triton_backend_setup.sh

* Prepare the triton model repo. Downloads the model files.
- $ cd /opt/nvidia/deepstream/deepstream/samples/
- $ ./prepare_ds_triton_model_repo.sh # prepare the triton model repo

* Launch the graph
- $ /opt/nvidia/graph-composer/execute_graph.sh deepstream-triton.yaml \
        deepstream-triton.parameters.jetson.yaml -d target_triton_aarch64.yaml

NOTE: Sometime there can be following error while running the graph.
   "unable to load backend library: /usr/lib/aarch64-linux-gnu/libgomp.so.1: cannot allocate memory in static TLS block"
To solve the issue the libgomp.so.1 needs to be preloaded which can be done as
follows before running the sample:
$ export LD_PRELOAD=/usr/lib/aarch64-linux-gnu/libgomp.so.1:$LD_PRELOAD

Steps for running in a container:
* Building a Jetson container on x86 host
- In ds_triton_container_builder_cfg_jetson.yaml, change <docker-container-registry>
  to a valid docker registry where containers can be pushed, and then build:
  $ container_builder -c ds_triton_container_builder_cfg_jetson.yaml \
        -d target_triton_aarch64.yaml

- Push the built image to the docker registry:
  $ docker push <docker-container-registry>:deepstream-triton-jetson

* Running the image on Jetson
  $ sudo docker pull <docker-container-registry>:deepstream-triton-jetson
  $ xhost +
  $ sudo docker run -it --rm --net=host --runtime nvidia -e DISPLAY=:0 \
        -v /tmp/.X11-unix/:/tmp/.X11-unix \
        -v /tmp/argus_socket:/tmp/argus_socket \
        <docker-container-registry>:deepstream-triton-jetson
