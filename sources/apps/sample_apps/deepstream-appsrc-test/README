*****************************************************************************
* Copyright (c) 2020-2023 NVIDIA Corporation.  All rights reserved.
*
* NVIDIA Corporation and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA Corporation is strictly prohibited.
*****************************************************************************

*****************************************************************************
                     deepstream-appsrc-test
                             README
*****************************************************************************

===============================================================================
1. Prerequisites:
===============================================================================

Please follow instructions in the apps/sample_apps/deepstream-app/README on how
to install the prequisites for Deepstream SDK, the DeepStream SDK itself and the
apps.

You must have the following development packages installed
    GStreamer-1.0
    GStreamer-1.0 Base Plugins
    GStreamer-1.0 gstrtspserver
    X11 client-side library

To install these packages, execute the following command:
   sudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev \
   libgstrtspserver-1.0-dev libx11-dev

This example can be configured to use either the nvinfer or the nvinferserver
element for inference.
If nvinferserver is selected, the Triton Inference Server is used for inference
processing. In this case, the example needs to be run inside the
DeepStream-Triton docker container for dGPU platforms. For Jetson platforms,
this example can be executed on target device directly or inside DeepStream L4T
Triton container. Please refer samples/configs/deepstream-app-triton/README for
the steps to download the container image and setup model repository.
===============================================================================
2. Purpose:
===============================================================================

This document shall describe about the sample deepstream-appsrc-test application.

It is meant to demonstrate how raw video frames acquired from outside DeepStream
can be fed to a DeepStream pipeline. It also demostrates how metadata can be accessed
via appsink and used outside deepstream.

===============================================================================
3. To compile:
===============================================================================

  $ Set CUDA_VER in the MakeFile as per platform.
      For Jetson, CUDA_VER=11.4
      For x86, CUDA_VER=12.1
  $ sudo make (sudo not required in case of docker containers)


===============================================================================
4. Usage:
===============================================================================

Creating raw video streams from Encoded streams:
Raw streams can be created using gst-launch-1.0. The pipeline is as follows:
  $ gst-launch-1.0 uridecodebin uri=<URI of file> ! nvvideoconvert !
      'video/x-raw, format=<Format of stream (example: I420, NV12, RGBA)>,
      width=<Width of stream>, height=<height of stream>' ! filesink location=test.raw

Ensure the directory where raw file needs to be saved has write permissions.
i.e. /opt/nvidia/deepstream/deepstream/samples/streams/ needs write permissions before executing
below sample pipeline.

  $ gst-launch-1.0 uridecodebin \
      uri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.mp4 \
      ! nvvideoconvert ! 'video/x-raw, format=I420, width=1280, height=720' \
      ! filesink location= /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.i420

To run the application with raw video stream:
  $ ./deepstream-appsrc-test <Raw video stream (example: YUV)> <width of stream>
      <height of stream> <expected FPS of stream> <format of stream (example: I420, NV12, RGBA)>
  e.g.
  $ ./deepstream-appsrc-test /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.i420 1280 720 30 I420

Use option "-t inferserver" to select nvinferserver as the inference plugin
  $ ./deepstream-appsrc-test -t inferserver <Raw video stream (example: YUV)> <width of stream>
      <height of stream> <expected FPS of stream> <format of stream (example: I420, NV12, RGBA)>
  e.g.
  $ ./deepstream-appsrc-test -t inferserver /opt/nvidia/deepstream/deepstream/samples/streams/sample_720p.i420 1280 720 30 I420

This sample uses appsrc APIs to push raw video frames read using fread () onto appsrc
component. From appsrc, the usual deepstream components are then used. Single primary
inferencing is used here. Then the buffers are sent via a tee to regular video rendering sink and
appsink. Appsink extracts buffer from sample and then obtains metadata information from it.

NOTE: This app supports only RGBA, NV12 and I420 raw video streams.

