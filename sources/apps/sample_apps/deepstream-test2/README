*****************************************************************************
* SPDX-FileCopyrightText: Copyright (c) 2018-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
* SPDX-License-Identifier: LicenseRef-NvidiaProprietary
*
* NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
* property and proprietary rights in and to this material, related
* documentation and any modifications thereto. Any use, reproduction,
* disclosure or distribution of this material and related documentation
* without an express license agreement from NVIDIA CORPORATION or
* its affiliates is strictly prohibited.
*****************************************************************************


*****************************************************************************
                     deepstream-test2-app
                            README
*****************************************************************************

===============================================================================
1. Prerequisites:
===============================================================================

Please follow instructions in the apps/sample_apps/deepstream-app/README on how
to install the prequisites for Deepstream SDK, the DeepStream SDK itself and the
apps.

You must have the following development packages installed
   GStreamer-1.0
   GStreamer-1.0 Base Plugins
   GStreamer-1.0 gstrtspserver
   X11 client-side library

To install these packages, execute the following command:
   sudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev \
   libgstrtspserver-1.0-dev libx11-dev

This example can be configured to use either the nvinfer or the nvinferserver
element for inference.
If nvinferserver is selected, the Triton Inference Server is used for inference
processing. In this case, the example needs to be run inside the
DeepStream-Triton docker container. Please refer
samples/configs/deepstream-app-triton/README for the steps to download the
container image and setup model repository.

===============================================================================
2. Purpose:
===============================================================================

In this sample, one instance of "nvinfer" or "nvinferserver" referred as the
pgie, is created. It uses a 4 class detector and it detects the classes
"Vehicle , RoadSign, TwoWheeler, Person". After this a "nvtracker" instance
is linked which tracks the objects detected by the pgie. After this 3 more
instances of "nvinfer" or "nvinferserver" referred to as sgie1, sgie2, sgie3
respectively are created.

===============================================================================
3. To compile:
===============================================================================

$ Set CUDA_VER in the MakeFile as per platform.
    For Jetson, CUDA_VER=11.4
    For x86, CUDA_VER=11.8
$ sudo make (sudo not required in case of docker containers)

===============================================================================
4. Usage:
===============================================================================

  Two ways to run the application:

  1.Run with the h264 elementary stream. In this method, user needs to modify the source
    code of deepstream-test2-app to configure pipeline properties.

    $ ./deepstream-test2-app <h264_elementary_stream>
  2.Run with the yml file. In this method, user needs to update the yml file to configure
    pipeline properties.

    $ ./deepstream-test2-app <yml file>
    e.g. ./deepstream-test2-app dstest2_config.yml

NOTE: To compile the sources, run make with "sudo" or root permission.

This document shall describe about the sample deepstream-test2 application.

It is meant for simple demonstration of how to use the various DeepStream SDK
elements in the pipeline and extract meaningful insights from a video stream.

This sample creates multiple instances of either "nvinfer" or "nvinferserver"
element for inference. Each instance of the "nvinfer" uses TensorRT API to infer
on frames/objects. Each instance of the "nvinferserver" uses the Triton Inference
Server to infer on frames/objects. Every instance is configured through its
respective config file. Using a correct configuration for a inference element
instance is therefore very important as considerable behaviors of the instance
are parameterized through these configs.

For reference, here are the config files used for this sample :
1. The 4-class detector (referred to as pgie in this sample) uses
    dstest2_pgie_config.yml / dstest2_pgie_nvinferserver_config.txt
2. The vehicle color classifier (referred to as sgie1 in this sample) uses
    dstest2_sgie1_config.yml / dstest2_sgie1_nvinferserver_config.txt
3. The vehicle make classifier (referred to as sgie2 in this sample) uses
    dstest2_sgie2_config.yml / dstest2_sgie2_nvinferserver_config.txt
4. The vehicle type classifier (referred to as sgie3 in this sample) uses
    dstest2_sgie3_config.yml / dstest2_sgie3_nvinferserver_config.txt
5. The tracker (referred to as nvtracker in this sample) uses
    dstest2_tracker_config.txt

The GIE configuration groups in the YAML configuration file of the application
can be used to set the inference plugin type (nvinfer or nvinferserver) and
corresponding plugin configuration file.

Each of the inference elements attach inference metadata to the buffer.
By attaching the probe function at the end of the pipeline, one can extract
meaningful information from these inferences. Please refer the
"osd_sink_pad_buffer_probe" function in the sample code.  For details on the
metadata format, refer to the file "gstnvdsmeta.h"
