*****************************************************************************
* Copyright (c) 2023 NVIDIA Corporation.  All rights reserved.
*
* NVIDIA Corporation and its licensors retain all intellectual property
* and proprietary rights in and to this software, related documentation
* and any modifications thereto.  Any use, reproduction, disclosure or
* distribution of this software and related documentation without an express
* license agreement from NVIDIA Corporation is strictly prohibited.
*****************************************************************************

*****************************************************************************
                  deepstream-multigpu-nvlink-test-app
                                README
*****************************************************************************

===============================================================================
1. Prerequisites:
===============================================================================

Please follow instructions in the apps/sample_apps/deepstream-app/README on how
to install the prerequisites for the Deepstream SDK, the DeepStream SDK itself,
and the apps.

You must have the following development packages installed
   GStreamer-1.0
   GStreamer-1.0 Base Plugins
   GStreamer-1.0 gstrtspserver
   X11 client-side library

To install these packages, execute the following command:
   sudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev \
   libgstrtspserver-1.0-dev libx11-dev

===============================================================================
2. Purpose:
===============================================================================

This document describes the sample deepstream-multigpu-nvlink-test application.

This sample demonstrate how to:

* Use gst-nvdsxfer plugin to simulate pipelines with NVLINK enabled multi-gpu
  setup to achieve better performance. User can use "position" param of nvxfer
  config section from dsmultigpu_config.yml file to simulate gst-nvxfer plugin
  supported various multi-gpu usecase pipelines.
* Use a uridecodebin so that any type of input (e.g. RTSP/File), any GStreamer
  supported container format, and any codec can be used as input.

===============================================================================
3. To compile:
===============================================================================

  $ Set CUDA_VER in the MakeFile as per platform.
      For x86, CUDA_VER=12.1
  $ sudo make

NOTE:
1. To compile the sources, run make with "sudo" or root permission.
2. deepstream-multigpu-nvlink-test application is not supported on Jetson Platforms.

===============================================================================
4. Usage:
===============================================================================

  Way to run the application:

  Run with the yml file. In this method, user needs to update the yml file to configure
  pipeline properties.

    $ ./deepstream-multigpu-nvlink-test <yml file>
    e.g. $ ./deepstream-multigpu-nvlink-test dsmultigpu_config.yml

  dsmultigpu_config.yml file provides following section for nvdsxfer plugin configuration.

  nvxfer:
    gpu-id: 1
    p2p-gpu-id: 0
    # nvdsxfer plugin Position in the pipeline
    # 0 : Decoder(s) -> Streammux -> PGIE -> nvxfer -> Tracker -> SGIE(s)
    # 1 : Decoder(s) -> Streammux -> nvxfer -> PGIE -> Tracker -> SGIE(s)
    # 2 : Decoder(s) -> Streammux -> PGIE -> Tracker -> nvxfer -> SGIE(s)
    position: 0

    NOTE: User need to update gpu-id for PGIE and Tracker corresponding to
          nvxfer position value used with dsmultigpu_config.yml

  * User need to specify "gpu-id" and "p2p-gpu-id" to enable NVLINK based
    data transfer between two dGPUs in the multi-gpu system.
    NOTE: Currently application can support data transfer between multi-gpu system with
          two dGPUs only. Data transfer between more than two dGPUs based multi-gpu system
          are not supported with the application.
  * To disable NVLINK based data transfer, set p2p-gpu-id: -1 which will instead use PCIe
    path for data transfer between two dGPUs with lower performance.
  * User need to select the "position" params to specify nvdsxfer plugin placement in
    the pipeline.

This sample accepts one or more H.265 video streams as input. It creates
a source bin for each input and connects the bins to an instance of the
"nvstreammux" element, which forms the batch of frames. The batch of
frames is fed to "nvinfer" for batched inferencing. PGIE output is given to
nvtracker for tracking and then feed to multiple SGIEs for secondary inferencing.
The batched buffer then can be composited into a 2D tile array using "nvmultistreamtiler"
to display osd output using supported videosink. nvdsxfer plugin can be inserted
in between two elements, for supported position as described with dsmultigpu_config.yml,
to simulate multi-gpu based usecase pipelines which make use of NVLINK to achieve better
performance.

The "width" and "height" properties must be set on the stream-muxer to set the
output resolution. If the input frame resolution is different from
stream-muxer's "width" and "height", the input frame will be scaled to muxer's
output resolution.

The stream-muxer waits for a user-defined timeout before forming the batch. The
timeout is set using the "batched-push-timeout" property. If the complete batch
is formed before the timeout is reached, the batch is pushed to the downstream
element. If the timeout is reached before the complete batch can be formed
(which can happen in case of rtsp sources), the batch is formed from the
available input buffers and pushed. Ideally, the timeout of the stream-muxer
should be set based on the framerate of the fastest source. It can also be set
to -1 to make the stream-muxer wait infinitely.

The "nvmultistreamtiler" composite streams based on their stream-ids in
row-major order (starting from stream 0, left to right across the top row, then
across the next row, etc.).

NOTE: To reuse engine files generated in previous runs, update the
model-engine-file parameter in the nvinfer config file to an existing engine file
